{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReplicateError",
     "evalue": "You did not pass an authentication token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mreplicate\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mreplicate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaibhavs10/incredibly-fast-whisper:3ab86df6c8f54c11309d4d1f930ac292bad43ace52d10c80d87eb258b3c9f79c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReal time AI Conversation Co-pilot on your phone, Crazy or Creepy_.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiarise_audio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/personal/my_repos/youtube_metadata/.venv/lib/python3.10/site-packages/replicate/client.py:157\u001b[0m, in \u001b[0;36mClient.run\u001b[0;34m(self, ref, input, **params)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     ref: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28minput\u001b[39m: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Unpack[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions.CreatePredictionParams\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Any, Iterator[Any]]:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Run a model and wait for its output.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/my_repos/youtube_metadata/.venv/lib/python3.10/site-packages/replicate/run.py:40\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m     37\u001b[0m version, owner, name, version_id \u001b[38;5;241m=\u001b[39m identifier\u001b[38;5;241m.\u001b[39m_resolve(ref)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m owner \u001b[38;5;129;01mand\u001b[39;00m name:\n\u001b[1;32m     44\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     45\u001b[0m         model\u001b[38;5;241m=\u001b[39m(owner, name), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m     46\u001b[0m     )\n",
      "File \u001b[0;32m~/personal/my_repos/youtube_metadata/.venv/lib/python3.10/site-packages/replicate/prediction.py:373\u001b[0m, in \u001b[0;36mPredictions.create\u001b[0;34m(self, version, input, **params)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03mCreate a new prediction for the specified model version.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m body \u001b[38;5;241m=\u001b[39m _create_prediction_body(\n\u001b[1;32m    369\u001b[0m     version,\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    372\u001b[0m )\n\u001b[0;32m--> 373\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _json_to_prediction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client, resp\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/personal/my_repos/youtube_metadata/.venv/lib/python3.10/site-packages/replicate/client.py:87\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m     86\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(method, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 87\u001b[0m     \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/personal/my_repos/youtube_metadata/.venv/lib/python3.10/site-packages/replicate/client.py:368\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_for_status\u001b[39m(resp: httpx\u001b[38;5;241m.\u001b[39mResponse) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m--> 368\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReplicateError(resp\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mReplicateError\u001b[0m: You did not pass an authentication token"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "\n",
    "output = replicate.run(\n",
    "    \"vaibhavs10/incredibly-fast-whisper:3ab86df6c8f54c11309d4d1f930ac292bad43ace52d10c80d87eb258b3c9f79c\",\n",
    "    input={\n",
    "        \"task\": \"transcribe\",\n",
    "        \"audio\": \"Real time AI Conversation Co-pilot on your phone, Crazy or Creepy_.mp3\",\n",
    "        \"language\": \"None\",\n",
    "        \"timestamp\": \"chunk\",\n",
    "        \"batch_size\": 64,\n",
    "        \"diarise_audio\": False\n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from rich import print as rprint\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    url = \"http://localhost:8000/transcribe/\"  # Adjust this URL based on your setup\n",
    "    files = {'file': open(file_path, 'rb')}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Transcription successful!\")\n",
    "        data = response.json()\n",
    "        # You can also print additional information, like detected language, if your response includes that\n",
    "    else:\n",
    "        print(\"Failed to transcribe audio. Status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"audio.mp3\"  # Adjust this to the path of your audio file\n",
    "    response = transcribe_audio(file_path)\n",
    "    with open(\"transcription.txt\", \"w\") as f:\n",
    "        f.write(response['segments'][0]['text'])\n",
    "    rprint(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rich import print as rprint\n",
    "import json\n",
    "\n",
    "import prompts\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "key = \"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\"\n",
    "\n",
    "\n",
    "class JsonCreator(BaseModel):\n",
    "    json_object: str\n",
    "\n",
    "\n",
    "def summarize_transcription_for_thumbnail(transcription: str, model: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a video transcription and returns a concise summary for use in YouTube thumbnail prompt creation.\n",
    "\n",
    "    Args:\n",
    "    - transcription (str): The transcription text of the YouTube video.\n",
    "    - model_name (str): The name of the model to use for summarization.\n",
    "\n",
    "    Returns:\n",
    "    - str: A concise summary suitable for creating a YouTube video thumbnail.\n",
    "    \"\"\"\n",
    "        #chat = ChatAnthropic(temperature=0, anthropic_api_key=key, model_name=\"claude-instant-1.2\")\n",
    "\n",
    "    \n",
    "    \n",
    "    analysis_type = \"json_creator\"\n",
    "    role = \"Analyst\"\n",
    "    chat = ChatOllama(model=model_name,base_url='http://192.168.86.29:11434')\n",
    "    #chat = ChatAnthropic(temperature=0, anthropic_api_key=key, model_name=model)\n",
    "    # Generate the system prompt based on the analysis type and role\n",
    "    message = prompts.get_system_prompt(analysis_type, role)\n",
    "    rprint(message)\n",
    "\n",
    "\n",
    "    system = (\n",
    "        message\n",
    "    )\n",
    "    human = \"{text}\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "    chain = prompt | chat \n",
    "        # Define the analysis type and role for the prompt\n",
    "    # Extract the response content\n",
    "    response = chain.invoke({\"text\": transcription})\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    transcription = 'Second President of the United States'\n",
    "    model_name = input(\"Enter model name: \")\n",
    "    summary = summarize_transcription_for_thumbnail(transcription, model_name)\n",
    "    rprint(f\"Video Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"dolphin-mistral\",base_url='http://192.168.86.29:11434')\n",
    "\n",
    "with open(\"transcription.txt\", \"r\") as f:\n",
    "        transcription = f.read()\n",
    "        \n",
    "# Define your desired data structure.\n",
    "class TextSummarization(BaseModel):\n",
    "    summary: str = Field(description=\"The summary of the video transcription.\")\n",
    "    confidence_score: float = Field(\n",
    "        description=\"The confidence score of the summary's accuracy.\"\n",
    "    )\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=TextSummarization)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" {format_instructions}\\n{transcription}\\n\"\"\",\n",
    "    input_variables=[\"transcription\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "summary = chain.invoke({\"transcription\": transcription})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "import prompts\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url='http://192.168.86.29:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "\n",
    "analysis_type = \"president_speech_creator\"\n",
    "role = \"Analyst\"\n",
    "\n",
    "\n",
    "client = ollama_client\n",
    "model = input(\"Enter model name: \")\n",
    "message = prompts.get_system_prompt(analysis_type, role)\n",
    "rprint(message)\n",
    "\n",
    "# Create a chat completion\n",
    "chat_completion = client.chat.completions.create(\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": message},\n",
    "        {\"role\": \"user\", \"content\": \"Need speech about implications of AI on the United States job loss\"},\n",
    "    ],\n",
    "    \n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Extract the response content\n",
    "response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "# Check if the response content is already a dictionary (JSON object)\n",
    "if isinstance(response_content, dict):\n",
    "    # Directly work with the JSON object\n",
    "    rprint('Response is already a JSON object')\n",
    "else:\n",
    "    # Assuming the response content is a string in JSON format\n",
    "    try:\n",
    "        # Parse the JSON content to a Python dictionary\n",
    "        rprint(f'Response is not a JSON object. Attempting to parse JSON from string: {type(response_content)}')\n",
    "        json_object = json.loads(response_content)\n",
    "        rprint(\"SUCCESS\")\n",
    "        rprint(json_object)\n",
    "    except json.JSONDecodeError as e:\n",
    "        rprint(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "df = pd.json_normalize(json_object)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from rich import print as rprint\n",
    "import uuid\n",
    "\n",
    "def download_image(image_url, save_path):\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        rprint(f\"Image downloaded successfully: {save_path}\")\n",
    "    else:\n",
    "        rprint(\"Failed to download image.\")\n",
    "\n",
    "def generate_thumbnail(description):\n",
    "    url = \"http://localhost:8001/generate-thumbnail/\"  # Adjust this URL based on your setup\n",
    "    payload = {'description': description}\n",
    "    response = requests.post(url, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        rprint(\"Thumbnail generation successful!\")\n",
    "        data = response.json()\n",
    "        image_url = data['data'][0]['url']\n",
    "        rprint(\"Thumbnail URL:\", image_url)\n",
    "        rprint(\"Revised Prompt:\", data['data'][0]['revised_prompt'])\n",
    "        \n",
    "        # Specify the path where you want to save the image\n",
    "        save_path = str(uuid.uuid4()) + \"-downloaded_thumbnail.jpg\"\n",
    "        download_image(image_url, save_path)\n",
    "    else:\n",
    "        rprint(\"Failed to generate thumbnail. Status code:\", response.status_code)\n",
    "        rprint(\"Response:\", response.text)\n",
    "\n",
    "    return save_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    description = json_object['thumbnail_prompt']\n",
    "    save_path = generate_thumbnail(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Replace 'path_to_your_image.png' with the actual path to your image file\n",
    "Image(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualContentSynthesisModel = dynamic_models['visual_content_synthesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualContentSynthesisModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import JSON_SCHEMAS, create_pydantic_model\n",
    "from rich import print as rprint\n",
    "\n",
    "# Dynamically create models for each schema\n",
    "dynamic_models = {}\n",
    "for schema_name, schema_fields in JSON_SCHEMAS.items():\n",
    "    model = create_pydantic_model(schema_name, schema_fields)\n",
    "    dynamic_models[schema_name] = model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Example usage\n",
    "VisualContentSynthesisModel = dynamic_models['visual_content_synthesis']\n",
    "try:\n",
    "    example_instance = VisualContentSynthesisModel(\n",
    "        key_visual_elements=[\"Sunset\", \"Mountain\"],\n",
    "        themes=[\"Adventure\", \"Freedom\"],\n",
    "        representative_moments=[\"Climbing the peak\"],\n",
    "        suggested_concepts=[\"Explore the world\"],\n",
    "        color_scheme=\"Warm Earth Tones\",\n",
    "        emotional_tone=\"Inspiring\"\n",
    "    )\n",
    "    rprint(example_instance)\n",
    "except ValidationError as e:\n",
    "    print(e.json())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import JSON_SCHEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "VisualContentSynthesisModel = dynamic_models['visual_content_synthesis']\n",
    "try:\n",
    "    example_instance = VisualContentSynthesisModel(\n",
    "        key_visual_elements=[\"Sunset\", \"Mountain\"],\n",
    "        themes=[\"Adventure\", \"Freedom\"],\n",
    "        representative_moments=[\"Climbing the peak\"],\n",
    "        suggested_concepts=[\"Explore the world\"],\n",
    "        color_scheme=\"Warm Earth Tones\",\n",
    "        emotional_tone=\"Inspiring\"\n",
    "    )\n",
    "    print(example_instance)\n",
    "except ValidationError as e:\n",
    "    print(e.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
